{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Landing - Single DQN\n",
    "Code adapted from:https://github.com/fakemonk1/Reinforcement-Learning-Lunar_Lander\n",
    "\n",
    "## 1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import relu, linear\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error as mse\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a DQN Agent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, learning_rate, gamma, epsilon, epsilon_decay):\n",
    "        #set parameters\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.total_step = 0\n",
    "        self.step_size = 5\n",
    "        self.rewards_list = []\n",
    "        self.hidden_dim = 128\n",
    "        self.replay_memory_buffer = deque(maxlen=1000000)\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.n_action = env.action_space.n\n",
    "        self.n_observation = env.observation_space.shape[0]\n",
    "        self.model = self.reset_model()\n",
    "        #self.target_model = self.reset_model() # create a second network\n",
    "        self.render = True\n",
    "    \n",
    "    #def update_target(self):\n",
    "        #return self.target_model.set_weights(self.model.get_weights())\n",
    "    def reset_model(self):\n",
    "        #create the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.n_observation, activation=relu)) \n",
    "        model.add(Dense(self.hidden_dim, activation=relu))\n",
    "        model.add(Dense(self.n_action, activation=linear))\n",
    "        model.compile(loss=mse, optimizer=Adam(lr=self.learning_rate)) # compile the model\n",
    "        return model\n",
    "\n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.n_action)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        env = gym.make('LunarLander-v2')\n",
    "        for episode in tqdm (range(num_episodes)):\n",
    "            state = env.reset()\n",
    "            reward_one = 0\n",
    "            num_steps = 1000\n",
    "            state = np.reshape(state, [1, self.n_observation])\n",
    "            for t in range(num_steps):\n",
    "                if self.render:\n",
    "                    env.render()\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.n_observation])\n",
    "                # save memory\n",
    "                self.add_memory(state, action, reward, next_state, done)\n",
    "                # reward\n",
    "                reward_one += reward\n",
    "                state = next_state\n",
    "                self.update_net()\n",
    "                self.total_step += 1\n",
    "                if done:\n",
    "                    break\n",
    "            self.rewards_list.append(reward_one)\n",
    "\n",
    "            # Decay the epsilon after each experience completion\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            #if episode % 10 == 0:\n",
    "            #self.update_target() # update the network \n",
    "                \n",
    "            # Avg reward\n",
    "            rewards_mean = np.mean(self.rewards_list[-100:])\n",
    "            if rewards_mean > 200:\n",
    "                print(\"DQN Training Finish...\")\n",
    "                break\n",
    "            print(\"\\t Episode :\", episode, \"\\t  Reward: \",reward_one, \"\\t Average Reward: \",rewards_mean)\n",
    "\n",
    "\n",
    "    def update_net(self):# replay\n",
    "        # check if training is needed\n",
    "        if np.mean(self.rewards_list[-10:]) > 180 or (self.total_step % self.step_size) != 0 or len(self.replay_memory_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        sample_data = random.sample(self.replay_memory_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, done = self.reshape_data(sample_data)\n",
    "        q_target = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done)\n",
    "        # turn single to double\n",
    "        #q_target = rewards + self.gamma * (np.amax(self.target_model.predict_on_batch(next_states), axis=1)) * (1 - done)\n",
    "        \n",
    "        q_total = self.model.predict_on_batch(states)\n",
    "        idx = np.array([i for i in range(self.batch_size)])\n",
    "        q_total[[idx], [actions]] = q_target\n",
    "        self.model.fit(states, q_total, epochs=1, verbose=0)\n",
    "\n",
    "    def reshape_data(self, random_sample):\n",
    "        states = np.array([i[0] for i in random_sample])\n",
    "        actions = np.array([i[1] for i in random_sample])\n",
    "        rewards = np.array([i[2] for i in random_sample])\n",
    "        next_states = np.array([i[3] for i in random_sample])\n",
    "        done = np.array([i[4] for i in random_sample])\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        return np.squeeze(states), actions, rewards, next_states, done\n",
    "\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "def test(lr):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(21)\n",
    "    np.random.seed(21)\n",
    "    lr = lr\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    gamma = 0.99\n",
    "    training_episodes = 1000\n",
    "    model = DQN(env, lr, gamma, epsilon, epsilon_decay)\n",
    "    model.train(training_episodes)\n",
    "    save_dir = \"saved_models\"\n",
    "    model.save(save_dir + str(lr) + \"trained_model.h5\")\n",
    "    pickle.dump(model.rewards_list, open(save_dir + str(lr) + \"train_rewards_list.p\", \"wb\"))\n",
    "    return model.rewards_list\n",
    "\n",
    "#rewards_list1 = test(0.0001)\n",
    "rewards_list2 = test(0.0003)\n",
    "#rewards_list3 = test(0.0005)\n",
    "#rewards_list4 = test(0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot - Single DQN Agent Learning Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#plt.title(\"Learning Rate for single DQN\")\n",
    "#plt.plot(rewards_list1,label = 'LR=0.0001')\n",
    "#plt.plot(rewards_list2, 'blue',label = 'LR=0.0003')\n",
    "#plt.plot(rewards_list3, 'yellow',label = 'LR=0.0005')\n",
    "#plt.plot(rewards_list4, 'green',label = 'LR=0.001')\n",
    "#plt.legend()\n",
    "#plt.xlabel(\"Episode\")\n",
    "#plt.ylabel(\"Mean_Reward\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Landing - Double DQN\n",
    "\n",
    "Code adapted from: https://github.com/anh-nn01/Lunar-Lander-Double-Deep-Q-Networks \\\n",
    "Reference paper: https://arxiv.org/abs/1509.06461\n",
    "\n",
    "## 1. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Double DQN Agent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, gamma, lr, input_size,  number_of_actions):\n",
    "        \n",
    "        # Parameters        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.lr = lr\n",
    "        self.batch_size = 64\n",
    "        self.memory_space = deque(maxlen = 1000000)\n",
    "        self.input_size = input_size\n",
    "        self.number_of_actions = number_of_actions\n",
    "        \n",
    "        \n",
    "        # Initialise dense network for policy\n",
    "        \n",
    "        self.policy_network = Sequential()\n",
    "        self.policy_network.add(Dense(128, input_dim = self.input_size, activation = \"relu\"))\n",
    "        self.policy_network.add(Dense(128 , activation = \"relu\"))\n",
    "        self.policy_network.add(Dense(self.number_of_actions, activation = \"linear\"))\n",
    "        self.policy_network.compile(loss = \"mean_squared_error\", optimizer = Adam(self.lr))\n",
    "        \n",
    "        # Initialise dense Target Network\n",
    "        \n",
    "        self.target_network = Sequential()\n",
    "        self.target_network.add(Dense(128, input_dim = self.input_size, activation = \"relu\"))\n",
    "        self.target_network.add(Dense(128 , activation = \"relu\"))\n",
    "        self.target_network.add(Dense(self.number_of_actions, activation = \"linear\"))\n",
    "        self.target_network.compile(loss = \"mean_squared_error\", optimizer = Adam(self.lr))\n",
    "         \n",
    "        self.update_network()\n",
    "        \n",
    "       \n",
    "    def memory_step(self, state, action, reward, next_state, done):\n",
    "        self.memory_space.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def update_network(self):\n",
    "        return self.target_network.set_weights(self.policy_network.get_weights())\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        # epsilon greedy aproach\n",
    "        if np.random.uniform(0.0, 1.0) < self.epsilon:\n",
    "            action = np.random.choice(self.number_of_actions)\n",
    "            \n",
    "        else:\n",
    "            state = np.reshape(state, [1,self.input_size])\n",
    "            # show Q values in a state\n",
    "            Qs = self.policy_network.predict(state) \n",
    "            # return action with maximum value\n",
    "            action = np.argmax(Qs[0])\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        \n",
    "        # Choose a sample from batch\n",
    "        current_batch_size = min(len(self.memory_space), self.batch_size)\n",
    "        sample_batch = random.sample(self.memory_space, current_batch_size)\n",
    "        \n",
    "        # preprocessing of samples\n",
    "        sample_states = np.ndarray(shape = (current_batch_size, self.input_size))\n",
    "        sample_actions = np.ndarray(shape = (current_batch_size, 1))\n",
    "        sample_rewards = np.ndarray(shape = (current_batch_size, 1))\n",
    "        sample_next_states = np.ndarray(shape = (current_batch_size, self.input_size))\n",
    "        sample_dones = np.ndarray(shape = (current_batch_size, 1))\n",
    "\n",
    "        index = 0\n",
    "        \n",
    "        for exp in sample_batch:\n",
    "            sample_states[index] = exp[0]\n",
    "            sample_actions[index] = exp[1]\n",
    "            sample_rewards[index] = exp[2]\n",
    "            sample_next_states[index] = exp[3]\n",
    "            sample_dones[index] = exp[4]\n",
    "            index += 1\n",
    "            \n",
    "        sample_next = self.target_network.predict(sample_next_states)        \n",
    "        # Q values for terminal states back to 0.\n",
    "        sample_next = sample_next * (np.ones(shape = sample_dones.shape) - sample_dones)\n",
    "        # choose max action for each state\n",
    "        sample_next = np.max(sample_next, axis=1)\n",
    "        sample_Qs = self.policy_network.predict(sample_states)\n",
    "        \n",
    "        for i in range(current_batch_size):\n",
    "            a = sample_actions[i,0]\n",
    "            sample_Qs[i,int(a)] = sample_rewards[i] + self.gamma * sample_next[i]\n",
    "            \n",
    "        q_target = sample_Qs    \n",
    "        self.policy_network.fit(sample_states, q_target, epochs = 1, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Lunar Landing Environment and Play Episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for playing episodes          \n",
    "def Main(): \n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    agent = Agent(env, gamma = 0.99, lr = 0.0003, input_size = 8, number_of_actions = 4)\n",
    "    avg = deque(maxlen=100)\n",
    "    number_of_episodes = 1000\n",
    "    step=0  \n",
    "    scores = []\n",
    "    avg_scores = []\n",
    "    \n",
    "    for episode in range(number_of_episodes):        \n",
    "        done = False\n",
    "        total = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done: \n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total+= reward\n",
    "            env.render()\n",
    "\n",
    "            agent.memory_step(state, action, reward, next_state, done)\n",
    "            agent.learn()\n",
    "\n",
    "            state = next_state\n",
    "            step +=1\n",
    "            \n",
    "        avg.append(total)     \n",
    "        avg_scores.append(np.mean(avg))\n",
    "        scores.append(total)\n",
    "        \n",
    "        agent.update_network()\n",
    "        # epsilon decay\n",
    "        agent.epsilon = max(0.1, 0.995 * agent.epsilon)\n",
    "        print(\"Episode:\", episode, \"Reward:\", total)\n",
    "Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot - Double DQN Agent Learning Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title(\"Learning Curve - Double DQN\")\n",
    "#plt.xlabel(\"Episode\")\n",
    "#plt.ylabel(\"Reward\")\n",
    "#plt.plot(rewards)\n",
    "#plt.title(\"Average Learning Curve - Double DQN\")\n",
    "#plt.xlabel(\"Episode\")\n",
    "#plt.ylabel(\"Reward\")\n",
    "#plt.plot(aver_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Landing - Dueling DQN\n",
    "Code adapted from:https://github.com/philtabor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restard Kernel before running this part\n",
    "# otherwise tensorflow might throw an error due to mix of keras imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(keras.Model):\n",
    "    \n",
    "    def __init__(self,number_of_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        # Create dense layers with 128 units\n",
    "        self.first_layer = keras.layers.Dense(128, activation='relu')\n",
    "        self.second_layer = keras.layers.Dense(128, activation='relu')\n",
    "        \n",
    "        # Two layers specific to Dueling DQN architecture\n",
    "        self.values = keras.layers.Dense(1, activation=None)\n",
    "        self.advantages = keras.layers.Dense(number_of_actions, activation=None)\n",
    "    \n",
    "    # input to output throught a layers  \n",
    "    def call(self, state):\n",
    "        \n",
    "        z = self.first_layer(state)\n",
    "        z = self.second_layer(z)\n",
    "        values = self.values(z)\n",
    "        advantages = self.advantages(z)\n",
    "        \n",
    "        # Transformation function to combine output\n",
    "        Q = (values + (advantages - tf.math.reduce_mean(advantages, axis=1, keepdims=True)))\n",
    "        \n",
    "        return Q\n",
    "    \n",
    "    def advantage(self, state):\n",
    "        \n",
    "        z = self.first_layer(state)\n",
    "        z = self.second_layer(z)\n",
    "        advantages = self.advantages(z)\n",
    "        \n",
    "        return advantages\n",
    "    \n",
    "# Replay buffer realaying on https://github.com/philtabor implementation\n",
    "class Replay():\n",
    "    def __init__(self, maximum_size, input_shape):\n",
    "        \n",
    "        self.memory_size = maximum_size\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.memory_size, *input_shape), dtype=np.float32)        \n",
    "        self.state_memory_new = np.zeros ((self.memory_size, *input_shape), dtype=np.float32)\n",
    "        \n",
    "        self.action_memory = np.zeros(self.memory_size, dtype= np.int32)\n",
    "        self.reward_memory = np.zeros(self.memory_size, dtype= np.float32)\n",
    "        self.terminal_memory = np.zeros(self.memory_size, dtype = bool)\n",
    "        \n",
    "    def transition(self, state, action, reward, state_next, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.state_memory[index] = state\n",
    "        self.state_memory_new[index] = state_next\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.memory_counter +=1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "        batch = np.random.choice(max_memory, batch_size, replace = False)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        states_next = self.state_memory_new[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, states_next, dones\n",
    "    \n",
    "    \n",
    "class Agent():\n",
    "    def __init__(self, lr, gamma, number_of_actions, epsilon, batch_size, input_size, memory_size = 100000):\n",
    "        \n",
    "        # Parameters\n",
    "        self.actions = [i for i in range(number_of_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = 0.001\n",
    "        self.eps_min = 0.01\n",
    "        self.replace = 100\n",
    "        self.batch_size = batch_size\n",
    "        self.step_counter = 0\n",
    "        \n",
    "        self.memory = Replay(memory_size,input_size)\n",
    "        \n",
    "        \n",
    "        # initiate and compile double Q system\n",
    "        self.q_current = DuelingDQN(number_of_actions)\n",
    "        self.q_next = DuelingDQN(number_of_actions)\n",
    "\n",
    "        self.q_current.compile(optimizer=Adam(learning_rate=lr), loss = 'mean_squared_error')\n",
    "        self.q_next.compile(optimizer=Adam(learning_rate=lr),loss ='mean_squared_error')\n",
    "\n",
    "    def transition( self, state, action, reward, state_new, done):\n",
    "        self.memory.transition(state, action, reward, state_new, done)\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # Epsilon greedy choice\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            state = np.array([observation])\n",
    "            actions = self.q_current.advantage(state)\n",
    "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.memory_counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        if self.step_counter % self.replace == 0:\n",
    "            self.q_next.set_weights(self.q_current.get_weights())\n",
    "            \n",
    "        states, actions, rewards, states_new, dones =  self.memory.sample(self.batch_size)\n",
    "\n",
    "        pred_q = self.q_current(states)\n",
    "        next_q = self.q_next(states_new)\n",
    "        target_q = pred_q.numpy()\n",
    "        max_actions = tf.math.argmax(self.q_current(states_new),axis=1)\n",
    "\n",
    "        for index, terminal in enumerate(dones):\n",
    "            target_q[index,actions[index]] = rewards[index]+ \\\n",
    "            self.gamma*next_q[index,max_actions[index]]*(1-int(dones[index]))\n",
    "\n",
    "\n",
    "        self.q_current.train_on_batch(states, target_q)\n",
    "        self.epsilon = self.epsilon - self.eps_decay if self.epsilon > \\\n",
    "                        self.eps_min else self.eps_min\n",
    "\n",
    "        self.step_counter += 1\n",
    "\n",
    "    \n",
    "# Main loop for playing episodes          \n",
    "def Main():\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    agent = Agent(lr = 0.0003, gamma = 0.99,number_of_actions = 4, epsilon=1,  batch_size = 64, input_size = [8])\n",
    "    number_of_episodes = 1000\n",
    "\n",
    "    scores = []\n",
    "    a_score = [] \n",
    "\n",
    "    for episode in range(number_of_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.transition(state, action, reward, next_state,done)\n",
    "            state = next_state\n",
    "            agent.learn()\n",
    "\n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        a_score.append(avg_score)\n",
    "        print('episode ', episode, 'score %.1f' %score,\n",
    "               'average score %.1f'% avg_score)\n",
    "        \n",
    "    f = open('scores.p', 'wb')\n",
    "    pickle.dump(scores, f)\n",
    "    f.close()\n",
    "    \n",
    "    m = open('avg_scores.p', 'wb')\n",
    "    pickle.dump(a_score, m)\n",
    "    m.close()\n",
    "\n",
    "Main()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
