{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fbbb4c579309cc8ef75c0e632529bc7",
     "grade": false,
     "grade_id": "cell-33b0e4dce2016840",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Coursework Part 1: Value Iteration\n",
    "\n",
    "In this exercise, you will implement the value iteration algorithm for three closely related, but different, gridworld environments.\n",
    "\n",
    "**Total number of marks:** 20 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include **all** of your source code. Please **do not change the file name or compress/zip your submission**. Please do not include any identifying information on the files you submit. This coursework will be marked **anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and **write your own code**.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, **please ensure that you follow these instructions**. Not doing so will cause the automarker to reject your work, and will assign you a score of zero for that question. **If the automarker rejects your work because you have not followed the instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please **do not use any non-standard, third-party libraries** apart from numpy and matplotlib. **If we are unable to run your code because you have used unsupported external libraries, you may not get any credit for your work.**\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel â†’ Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **1 minute** on the University's computers. Otherwise, you may not get credit for your work. You can run your code on the university's computers remotely using [UniDesk](https://bath.topdesk.net/tas/public/ssp/content/detail/knowledgeitem?unid=ff3266344c1d4eb2acb227cc9e3e1eee)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1f63de0fdcf845091a97902c847d2e",
     "grade": false,
     "grade_id": "cell-49e38b6d0da7d1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "In this coursework, you will implement the Value Iteration algorithm to compute an optimal policy for three different (but closely related) Markov Decision Processes. For your reference, the pseudo-code for the Value Iteration algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 2018, pp. 83).\n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The symbol $r$ represents the immediate reward on transition from state $s$ to the next state $s'$ via action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The three problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this kind of environment from the lectures. The grid squares in the figure are numbered as shown. In all exercises, the following are true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are *west*, *north*, *south*, and *east*. If the direction of movement is blocked by a wall (for example, if the agent executes action south at grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold (from a neighbouring grid square), the agent collects the gold. Note that, in order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square.\n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains a bomb (from a neighbouring grid square), the agent activates the bomb. \n",
    "\n",
    "**Terminal states:** The game terminates when all gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to $1 \\times 10 ^{-10}$. You can express that as `1e-10` in Python. \n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Do not use discounting (that is, set $\\gamma=1$).\n",
    "\n",
    "Use the following reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold (from a neighbouring grid square) is $-1 + 10 = +9$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a42c1c34d1fb04bef22da8276ff9780c",
     "grade": false,
     "grade_id": "cell-bb45c706447879a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Deterministic Environment (0 Marks)\n",
    "\n",
    "In this exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Your need to produce two one-dimensional numpy arrays with names `policy` and `v`. Both arrays should have a length of 25, with the element at index $i$ representing grid cell $i$ (see figure above). Both arrays should be accessible in the \"solution cell\" below!\n",
    "\n",
    "The array `policy` should be a numpy array of strings that specifies an optimal action at each grid location. Please use the abbreviations `\"n\"`, `\"e\"`, `\"s\"`, and `\"w\"` for the four actions. As an example, the value of `policy` at index `0` needs to give `\"n\"`, if _north_ is an optimal action in cell 0. The policy for a terminal state can be any action. If there are multiple optimal actions from a state, any optimal action will be considered as a correct answer. \n",
    "\n",
    "The array `v` should be an array of floats that contains the expected return at each grid square (that is, the state value under the optimal policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea5b17f9f7aea9b1f962fc29862764bc",
     "grade": false,
     "grade_id": "cw1_value_iteration_deterministic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.random_move_probability = 0.2\n",
    "\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.num_fields = self.num_cols * self.num_rows\n",
    "\n",
    "        self.rewards = np.array([-1]*self.num_fields)\n",
    "        self.rewards[self.bomb_positions] += self.bomb_reward\n",
    "        self.rewards[self.gold_positions] += self.gold_reward\n",
    "\n",
    "        self.step = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def make_step(self, action_index):\n",
    "        \"\"\"\n",
    "        Given an action, make state transition and observe reward.\n",
    "\n",
    "        :param action_index: an integer between 0 and the number of actions (4 in Gridworld).\n",
    "        :return: (reward, new_position)\n",
    "            WHERE\n",
    "            reward (float) is the observed reward\n",
    "            new_position (int) is the new position of the agent\n",
    "        \"\"\"\n",
    "        # Randomly sample action_index if world is stochastic\n",
    "        action = self.actions[action_index]\n",
    "\n",
    "        # Determine new position and check whether the agent hits a wall.\n",
    "        old_position = self.agent_position\n",
    "        new_position = self.agent_position\n",
    "        if action == \"n\":\n",
    "            candidate_position = old_position + self.num_cols\n",
    "            if candidate_position < self.num_fields:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"e\":\n",
    "            candidate_position = old_position + 1\n",
    "            if candidate_position % self.num_cols > 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                new_position = candidate_position\n",
    "        elif action == \"s\":\n",
    "            candidate_position = old_position - self.num_cols\n",
    "            if candidate_position >= 0:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"w\":  # \"LEFT\"\n",
    "            candidate_position = old_position - 1\n",
    "            if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "                new_position = candidate_position\n",
    "        else:\n",
    "            raise ValueError('Action was mis-specified!')\n",
    "\n",
    "        # Update the environment state\n",
    "        self.agent_position = new_position\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.rewards[self.agent_position]\n",
    "        return reward, new_position\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # The following statement returns a boolean. It is 'True' when the agent_position\n",
    "        # coincides with any bomb_positions or gold_positions.\n",
    "        return self.agent_position in np.append(self.bomb_positions, self.gold_positions)\n",
    "\n",
    "\n",
    "class TableAgent(object):\n",
    "    \"\"\"\n",
    "    Agent for Value Iteration\n",
    "    :param: env: environment, here we use Gridworld() as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # number of states\n",
    "        self.s_len = env.num_fields\n",
    "        # number of actions\n",
    "        self.a_len = env.num_actions\n",
    "        # reward for each state\n",
    "        self.r = list(env.rewards)\n",
    "        # the policy of each state,default is 0\n",
    "        self.pi = np.array([0]*self.s_len)\n",
    "        # actions - states transformation matrix\n",
    "        self.p = np.zeros([self.a_len, self.s_len, self.s_len], dtype=np.float)\n",
    "\n",
    "\n",
    "        # calculate the probability of s' after setting s and a.\n",
    "        for i, action in enumerate(env.actions):\n",
    "            # As this is not a stochastic environment, the probability of action is 1.\n",
    "            prob = 1.0\n",
    "            for src in range(0, self.s_len):\n",
    "                step = []\n",
    "                # For the position of gold and boom, there shouldn't be any other movement.\n",
    "                if src == 23 or src == 18:\n",
    "                    continue\n",
    "                # Add probability to actions - states transformation matrix.\n",
    "                if action == \"n\":\n",
    "                    candidate_position = src + env.num_cols\n",
    "                    if candidate_position >= env.num_fields:\n",
    "                        candidate_position = src\n",
    "                    step.append(candidate_position)\n",
    "                elif action == \"e\":\n",
    "                    candidate_position = src + 1\n",
    "                    if candidate_position % env.num_cols <= 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                        candidate_position = src\n",
    "                    step.append(candidate_position)\n",
    "                elif action == \"s\":\n",
    "                    candidate_position = src - env.num_cols\n",
    "                    if candidate_position < 0:\n",
    "                        candidate_position = src\n",
    "                    step.append(candidate_position)\n",
    "                elif action == \"w\":\n",
    "                    candidate_position = src - 1\n",
    "                    if candidate_position % env.num_cols >= env.num_cols - 1:\n",
    "                        candidate_position = src\n",
    "                    step.append(candidate_position)\n",
    "                for dst in step:\n",
    "                    self.p[i, src, dst] += prob\n",
    "        # value function\n",
    "        self.value_pi = np.zeros((self.s_len))\n",
    "        # states - actions function\n",
    "        self.value_q = np.zeros((self.s_len, self.a_len))\n",
    "        # discounting param\n",
    "        self.gamma = 1\n",
    "\n",
    "def value_iteration(agent, max_iter=-1):\n",
    "    \"\"\"\n",
    "    This function is the main process of value iteration\n",
    "\n",
    "    :param obj agent: TableAgent\n",
    "    :param int max_iter: maximum iteration.\n",
    "    :return (agent.pi,agent.value_pi), agent.pi: policy, agent.value_pi:value\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # save the value function (we need to compare with new one later.)\n",
    "        new_value_pi = np.zeros_like(agent.value_pi)\n",
    "        for i in range(0, agent.s_len):\n",
    "            value_sas = []\n",
    "            for j in range(0, agent.a_len):\n",
    "                # for each s and a, calculate value function\n",
    "                value_sa = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "                value_sas.append(value_sa)\n",
    "            # find the best value funcion from every col.\n",
    "            new_value_pi[i] = max(value_sas)\n",
    "\n",
    "        # if the difference between the new value function and old one is less than 1e10, stop iteration.\n",
    "        diff = np.sqrt(np.sum(np.power(agent.value_pi - new_value_pi, 2)))\n",
    "        if diff < 1e-10:\n",
    "            break\n",
    "        else:\n",
    "            agent.value_pi = new_value_pi\n",
    "        if iteration == max_iter:\n",
    "            break\n",
    "\n",
    "    #print('Iter {} rounds converge'.format(iteration))\n",
    "    for i in range(1, agent.s_len):\n",
    "        for j in range(0, agent.a_len):\n",
    "            # calculate the states - actions function\n",
    "            agent.value_q[i, j] = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "        # find the maximum action\n",
    "        max_act = np.argmax(agent.value_q[i, :])\n",
    "        agent.pi[i] = max_act\n",
    "    return agent.pi,agent.value_pi\n",
    "\n",
    "GW = Gridworld()\n",
    "Agent = TableAgent(GW)\n",
    "result = value_iteration(Agent)\n",
    "policy = np.array([Gridworld().actions[x] for x in result[0]])\n",
    "v = np.array(result[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f6a967ae761f92bf2a79af2be971939",
     "grade": false,
     "grade_id": "cell-02a5c34a5b828d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Example Test Cell**\n",
    "\n",
    "In the code cell below, we have provided an example of the type of test code that we will use to mark your work. In these tests, we first check that your `policy` and `v` variables are of the correct type, and then check that their values match the solution. In the future, the test code will be hidden from you.\n",
    "\n",
    "You must not delete or modify test cells in any way - any modifications you do make will be overwritten at run-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e11d89ae67ea8af3e01f18e48bd35f6",
     "grade": true,
     "grade_id": "cw1_value_iteration_deterministic_tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Solution policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Student's v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n",
      "Solution v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 1 is tested here.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We're giving you the solution values for Exercise 1, but not telling you how to compute them!\n",
    "solution_values = [3.0, 4.0, 5.0, 4.0, 5.0,\n",
    "                   4.0, 5.0, 6.0, 5.0, 6.0,\n",
    "                   5.0, 6.0, 7.0, 6.0, 7.0,\n",
    "                   6.0, 7.0, 8.0, 0.0, 8.0,\n",
    "                   7.0, 8.0, 9.0, 0.0, 9.0]\n",
    "solution_values = np.array(solution_values)\n",
    "\n",
    "# We're giving you the solution policy for Exercise 1, but not telling you how to compute it!\n",
    "solution_policy = [\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'e', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'e', 'e', 'e', 'n', 'w',]\n",
    "solution_policy = np.array(solution_policy)\n",
    "\n",
    "# Check that policy and v are numpy arrays.\n",
    "assert(isinstance(policy, np.ndarray))\n",
    "assert(isinstance(v, np.ndarray))\n",
    "\n",
    "# Check correct shapes of numpy arrays.\n",
    "assert(policy.shape == (25, ))\n",
    "assert(v.shape == (25, ))\n",
    "\n",
    "# Check whether the numpy arrays have the correct data types.\n",
    "assert(np.issubdtype(policy.dtype, np.unicode_)) # policy.dtype should be '<U1'\n",
    "assert(np.issubdtype(v.dtype, np.float64))\n",
    "\n",
    "# Check whether policy contains only \"n\", \"w\", \"s\", or \"e\" values.\n",
    "assert(np.all(np.isin(policy, np.array([\"n\", \"w\", \"s\", \"e\"]))))\n",
    "\n",
    "# Print student's solution and true solution for easier comparison / spotting of errors.\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))\n",
    "print(\"Solution policy:\")\n",
    "print(np.flip(solution_policy.reshape((5, 5)), 0))\n",
    "\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"Solution v:\")\n",
    "print(np.flip(solution_values.reshape((5, 5)), 0))\n",
    "\n",
    "# Compare policy (only on states that have a single optimal direction).\n",
    "states_to_check =  np.array([4, 9, 14, 17, 19, 20, 22, 24])\n",
    "np.testing.assert_array_equal(policy[states_to_check], solution_policy[states_to_check])\n",
    "\n",
    "# Compare state_values (also for terminal states --- they have to be zero!).\n",
    "states_to_check = np.delete(np.arange(25), np.array([18, 23]))\n",
    "np.testing.assert_array_almost_equal(v[states_to_check], solution_values[states_to_check], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db00038a44803605ff0fe1f8e0971696",
     "grade": false,
     "grade_id": "cell-05eb78b7446cb694",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Stochastic Environment (12 Marks)\n",
    "\n",
    "In this exercise, we introduce stochasticity into the environment. Now, the agent is not always able to execute its actions as intended.\n",
    "\n",
    "With probability 0.8, the agent moves as intended. However, with probability 0.2, it moves in a random direction.\n",
    "\n",
    "For example, from grid square 0, if the agent tries to move north, with probability 0.8 the action will work as intended. But with probability 0.2, the agent's motor control system will move it in a random direction (including north). So, it will randomly try to move west, east, north or south with probability 0.05 each. Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration.\n",
    "\n",
    "Your value iteration method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "596c020f7ef0ae864054c8dbc3496cc2",
     "grade": false,
     "grade_id": "cw1_value_iteration_stochastic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "[[6.04169329 7.28756636 8.61359951 0.         8.69262311]\n",
      " [4.86185111 5.99087587 6.37082431 0.         6.46721593]\n",
      " [3.67550938 4.69621388 4.99441863 3.2189158  5.10250988]\n",
      " [2.48699534 3.40945989 3.66922967 2.64122933 3.78610115]\n",
      " [1.35979208 2.19733672 2.42878751 1.57272161 2.55202451]]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.random_move_probability = 0.2\n",
    "\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.num_fields = self.num_cols * self.num_rows\n",
    "\n",
    "        self.rewards = np.array([-1]*self.num_fields)\n",
    "        self.rewards[self.bomb_positions] += self.bomb_reward\n",
    "        self.rewards[self.gold_positions] += self.gold_reward\n",
    "\n",
    "        self.step = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def make_step(self, action_index):\n",
    "        \"\"\"\n",
    "        Given an action, make state transition and observe reward.\n",
    "\n",
    "        :param action_index: an integer between 0 and the number of actions (4 in Gridworld).\n",
    "        :return: (reward, new_position)\n",
    "            WHERE\n",
    "            reward (float) is the observed reward\n",
    "            new_position (int) is the new position of the agent\n",
    "        \"\"\"\n",
    "        # Randomly sample action_index if world is stochastic\n",
    "        action = self.actions[action_index]\n",
    "\n",
    "        # Determine new position and check whether the agent hits a wall.\n",
    "        old_position = self.agent_position\n",
    "        new_position = self.agent_position\n",
    "        if action == \"n\":\n",
    "            candidate_position = old_position + self.num_cols\n",
    "            if candidate_position < self.num_fields:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"e\":\n",
    "            candidate_position = old_position + 1\n",
    "            if candidate_position % self.num_cols > 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                new_position = candidate_position\n",
    "        elif action == \"s\":\n",
    "            candidate_position = old_position - self.num_cols\n",
    "            if candidate_position >= 0:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"w\":  # \"LEFT\"\n",
    "            candidate_position = old_position - 1\n",
    "            if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "                new_position = candidate_position\n",
    "        else:\n",
    "            raise ValueError('Action was mis-specified!')\n",
    "\n",
    "        # Update the environment state\n",
    "        self.agent_position = new_position\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.rewards[self.agent_position]\n",
    "        return reward, new_position\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # The following statement returns a boolean. It is 'True' when the agent_position\n",
    "        # coincides with any bomb_positions or gold_positions.\n",
    "        return self.agent_position in np.append(self.bomb_positions, self.gold_positions)\n",
    "\n",
    "\n",
    "class TableAgent(object):\n",
    "    \"\"\"\n",
    "    Agent for Value Iteration\n",
    "    :param: env: environment, here we use Gridworld() as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # number of states\n",
    "        self.s_len = env.num_fields\n",
    "        # number of actions\n",
    "        self.a_len = env.num_actions\n",
    "        # reward for each state, shape=[1,self.s_len]\n",
    "        self.r = list(env.rewards)\n",
    "        # the policy of each state,default is 0,shape=[1,self.s_len]\n",
    "        self.pi = np.array([0]*self.s_len)\n",
    "        # actions - states transformation matrix ,shape=[self.a_len, self.s_len, self.s_len]\n",
    "        self.p = np.zeros([self.a_len, self.s_len, self.s_len], dtype=np.float)\n",
    "        # value function\n",
    "        self.value_pi = np.zeros((self.s_len))\n",
    "        # states - actions function\n",
    "        self.value_q = np.zeros((self.s_len, self.a_len))\n",
    "        # discounting param\n",
    "        self.gamma = 1\n",
    "        # calculate states - actions function for each s and a\n",
    "        self.fresh_p(env)\n",
    "\n",
    "\n",
    "    def fresh_p(self,env):\n",
    "        \"\"\"\n",
    "        Calculate states - actions function for each s and a\n",
    "        :param: env: environment, here we use Gridworld() as input.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the probability of s' after setting s and a.\n",
    "        for i, action in enumerate(env.actions):\n",
    "            # 2 probabilities of action.\n",
    "            prob1 = 0.8\n",
    "            prob2 = 0.05 # (1-0.8)/4 = 0.05\n",
    "            for src in range(0, self.s_len):\n",
    "                step = []\n",
    "                # For the position of gold and boom, there shouldn't be any other movement.\n",
    "                if src == 23 or src == 18:\n",
    "                    continue\n",
    "                # Add probability to actions - states transformation matrix.\n",
    "                return_v = self.return_step_and_pro(action,src,env,prob1,prob2)\n",
    "                step = return_v[0]\n",
    "                pro = return_v[1]\n",
    "                for k in range(0,len(step)):\n",
    "                    self.p[i, src, step[k]] += pro[k]\n",
    "\n",
    "    def return_step_and_pro(self,action,src,env,prob1,prob2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param action: the action intended to do.\n",
    "        :param src:  current position.\n",
    "        :param env:  Gridworld.\n",
    "        :param prob1: the probability of doing the intended action.\n",
    "        :param prob2: (1-prob1)/4. The probability of randomly doing other actions.\n",
    "        :return: step: next position after doing action. pro: probability of doing that action.\n",
    "        \"\"\"\n",
    "        step = []\n",
    "        pro = []\n",
    "        if action == \"n\":\n",
    "            candidate_position = src + env.num_cols\n",
    "            if candidate_position >= env.num_fields:\n",
    "                candidate_position = src\n",
    "        elif action == \"e\":\n",
    "            candidate_position = src + 1\n",
    "            if candidate_position % env.num_cols <= 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                candidate_position = src\n",
    "        elif action == \"s\":\n",
    "            candidate_position = src - env.num_cols\n",
    "            if candidate_position < 0:\n",
    "                candidate_position = src\n",
    "        elif action == \"w\":\n",
    "            candidate_position = src - 1\n",
    "            if candidate_position % env.num_cols >= env.num_cols - 1:\n",
    "                candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "        pro.append(prob1)\n",
    "\n",
    "        candidate_position = src + env.num_cols\n",
    "        if candidate_position >= env.num_fields:\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        candidate_position = src + 1\n",
    "        if candidate_position % env.num_cols <= 0:  # The %-operator denotes \"modulo\"-division.\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        candidate_position = src - env.num_cols\n",
    "        if candidate_position < 0:\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        candidate_position = src - 1\n",
    "        if candidate_position % env.num_cols >= env.num_cols - 1:\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        pro.extend([prob2]*4)\n",
    "        return step,pro\n",
    "\n",
    "\n",
    "def value_iteration(agent, max_iter=-1):\n",
    "    \"\"\"\n",
    "    This function is the main process of value iteration\n",
    "\n",
    "    :param obj agent: TableAgent\n",
    "    :param int max_iter: maximum iteration.\n",
    "    :return (agent.pi,agent.value_pi), agent.pi: policy, agent.value_pi:value\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # save the value function (we need to compare with new one later.)\n",
    "        new_value_pi = np.zeros_like(agent.value_pi)\n",
    "        for i in range(0, agent.s_len):\n",
    "            value_sas = []\n",
    "            for j in range(0, agent.a_len):\n",
    "                # for each s and a, calculate value function\n",
    "                value_sa = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "                value_sas.append(value_sa)\n",
    "            # find the best value funcion from every col.\n",
    "            new_value_pi[i] = max(value_sas)\n",
    "\n",
    "        # if the difference between the new value function and old one is less than 1e10, stop iteration.\n",
    "        diff = np.sqrt(np.sum(np.power(agent.value_pi - new_value_pi, 2)))\n",
    "        if diff < 1e-10:\n",
    "            break\n",
    "        else:\n",
    "            agent.value_pi = new_value_pi\n",
    "        if iteration == max_iter:\n",
    "            break\n",
    "\n",
    "    for i in range(1, agent.s_len):\n",
    "        for j in range(0, agent.a_len):\n",
    "            # calculate the states - actions function\n",
    "            agent.value_q[i, j] = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "        # find the maximum action\n",
    "        max_act = np.argmax(agent.value_q[i, :])\n",
    "        agent.pi[i] = max_act\n",
    "    return agent.pi,agent.value_pi\n",
    "\n",
    "GW = Gridworld()\n",
    "Agent = TableAgent(GW)\n",
    "result = value_iteration(Agent)\n",
    "policy = np.array([Gridworld().actions[x] for x in result[0]])\n",
    "v = np.array(result[1])\n",
    "\n",
    "#print(np.flip(policy.reshape((5, 5)), 0))\n",
    "#print(np.flip(v.reshape((5, 5)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0152b199697503339f0fd7f0f935472",
     "grade": true,
     "grade_id": "cw1_value_iteration_stochastic_tests",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 2 is tested here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8fb50a4a7886296d52685a540d2c162",
     "grade": false,
     "grade_id": "cell-e0de56802818cf9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Stochastic Environment with Two Pieces of Gold (8 marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this exercise, we have modified the stochastic environment presented in exercise 2. A second piece of gold has been placed on grid square 12. The terminal state is reached only when **all** pieces of gold are collected or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy for this altered environment using Value Iteration.\n",
    "\n",
    "Hint: You will need to change your state representation in order to account for the additional piece of gold.\n",
    "\n",
    "Your method should output two one-dimensional numpy arrays with names `policy` and `v`, as in the previous exercises. These arrays should specify the expected return and an optimal policy at the corresponding grid sqaure **before any pieces of gold are collected or a bomb is activated.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "635f8795634a99a01b6cc3e5f28ecb3a",
     "grade": false,
     "grade_id": "cw1_value_iteration_two_gold",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'e' 'e' 'w' 'w']\n",
      " ['e' 's' 's' 'n' 'n']\n",
      " ['e' 'e' 'n' 'w' 'w']\n",
      " ['e' 'n' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'w']]\n",
      "[[10.65103994 11.79603433 13.00848756  4.28547547 12.97048714]\n",
      " [11.1861353  12.32932372 12.5121464   0.         10.61568556]\n",
      " [12.28702748 13.59365638  4.99441863 12.41930416 11.19974428]\n",
      " [11.17522837 12.35165962 13.59092292 12.28122217 11.051285  ]\n",
      " [10.06409806 11.17488271 12.28045984 11.10816476  9.99389366]]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.num_dimensions = 3\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.gold_positions = np.array([12,23,37,73])\n",
    "        self.bomb_positions = np.array([18,43,68])\n",
    "        self.random_move_probability = 0.2\n",
    "        self.end_positinos = [37,73,18,43,68]\n",
    "\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.num_fields = self.num_cols * self.num_rows * self.num_dimensions\n",
    "        self.num_cells = self.num_cols * self.num_rows\n",
    "\n",
    "        self.rewards = np.array([-1]*self.num_fields)\n",
    "        for bomb_position in self.bomb_positions:\n",
    "            self.rewards[bomb_position] += self.bomb_reward\n",
    "        for gold_position in self.gold_positions:\n",
    "            self.rewards[gold_position] += self.gold_reward\n",
    "\n",
    "        self.step = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def make_step(self, action_index):\n",
    "        \"\"\"\n",
    "        Given an action, make state transition and observe reward.\n",
    "\n",
    "        :param action_index: an integer between 0 and the number of actions (4 in Gridworld).\n",
    "        :return: (reward, new_position)\n",
    "            WHERE\n",
    "            reward (float) is the observed reward\n",
    "            new_position (int) is the new position of the agent\n",
    "        \"\"\"\n",
    "        # Randomly sample action_index if world is stochastic\n",
    "        action = self.actions[action_index]\n",
    "\n",
    "        # Determine new position and check whether the agent hits a wall.\n",
    "        old_position = self.agent_position\n",
    "        new_position = self.agent_position\n",
    "        if action == \"n\":\n",
    "            candidate_position = old_position + self.num_cols\n",
    "            if candidate_position < self.num_fields:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"e\":\n",
    "            candidate_position = old_position + 1\n",
    "            if candidate_position % self.num_cols > 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                new_position = candidate_position\n",
    "        elif action == \"s\":\n",
    "            candidate_position = old_position - self.num_cols\n",
    "            if candidate_position >= 0:\n",
    "                new_position = candidate_position\n",
    "        elif action == \"w\":  # \"LEFT\"\n",
    "            candidate_position = old_position - 1\n",
    "            if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "                new_position = candidate_position\n",
    "        else:\n",
    "            raise ValueError('Action was mis-specified!')\n",
    "\n",
    "        # Update the environment state\n",
    "        self.agent_position = new_position\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self.rewards[self.agent_position]\n",
    "        return reward, new_position\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # The following statement returns a boolean. It is 'True' when the agent_position\n",
    "        # coincides with any bomb_positions or gold_positions.\n",
    "        return self.agent_position in np.append(self.bomb_positions, self.gold_positions1,self.gold_positions2)\n",
    "\n",
    "\n",
    "class TableAgent(object):\n",
    "    \"\"\"\n",
    "    Agent for Value Iteration\n",
    "    :param: env: environment, here we use Gridworld() as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        # number of states\n",
    "        self.s_len = env.num_fields\n",
    "        # number of actions\n",
    "        self.a_len = env.num_actions\n",
    "        # reward for each state, shape=[1,self.s_len]\n",
    "        self.r = list(env.rewards)\n",
    "        # the policy of each state,default is 0,shape=[1,self.s_len]\n",
    "        self.pi = np.array([0]*self.s_len)\n",
    "        # actions - states transformation matrix ,shape=[self.a_len, self.s_len, self.s_len]\n",
    "        self.p = np.zeros([self.a_len, self.s_len, self.s_len], dtype=np.float)\n",
    "        # value function\n",
    "        self.value_pi = np.zeros((self.s_len))\n",
    "        # states - actions function\n",
    "        self.value_q = np.zeros((self.s_len, self.a_len))\n",
    "        # discounting param\n",
    "        self.gamma = 1\n",
    "        # calculate states - actions function for each s and a\n",
    "        self.fresh_p(env)\n",
    "\n",
    "    def fresh_p(self,env):\n",
    "        \"\"\"\n",
    "        Calculate states - actions function for each s and a\n",
    "        :param: env: environment, here we use Gridworld() as input.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the probability of s' after setting s and a.\n",
    "        for i, action in enumerate(env.actions):\n",
    "            # 2 probabilities of action.\n",
    "            prob1 = 0.8\n",
    "            prob2 = 0.05 # (1-0.8)/4 = 0.05\n",
    "            for src in range(0, self.s_len):\n",
    "                step = []\n",
    "                # For the position of gold and boom, there shouldn't be any other movement.\n",
    "                if src in env.end_positinos:\n",
    "                    continue\n",
    "\n",
    "                # Add probability to actions - states transformation matrix.\n",
    "                return_v = self.return_step_and_pro(action,src,env,prob1,prob2)\n",
    "                step = return_v[0]\n",
    "                pro = return_v[1]\n",
    "                for k in range(0,len(step)):\n",
    "                    self.p[i, src, step[k]] += pro[k]\n",
    "\n",
    "\n",
    "    def return_step_and_pro(self,action,src,env,prob1,prob2):\n",
    "        \"\"\"\n",
    "        :param action: the action intended to do.\n",
    "        :param src:  current position.\n",
    "        :param env:  Gridworld.\n",
    "        :param prob1: the probability of doing the intended action.\n",
    "        :param prob2: (1-prob1)/4. The probability of randomly doing other actions.\n",
    "        :return: step: next position after doing action. pro: probability of doing that action.\n",
    "        \"\"\"\n",
    "        step = []\n",
    "        pro = []\n",
    "\n",
    "        if src == 12:\n",
    "            src += 50\n",
    "        if src == 23:\n",
    "            src += 25\n",
    "\n",
    "        if 0 <= src < 25:\n",
    "            dimension = 1\n",
    "        elif 25 <= src < 50:\n",
    "            dimension = 2\n",
    "        elif 50 <= src < 75:\n",
    "            dimension = 3\n",
    "\n",
    "        if action == \"n\":\n",
    "            candidate_position = src + env.num_cols\n",
    "            if candidate_position >= env.num_cells * dimension:\n",
    "                candidate_position = src\n",
    "        elif action == \"e\":\n",
    "            candidate_position = src + 1\n",
    "            if candidate_position % env.num_cols <= 0:  # The %-operator denotes \"modulo\"-division.\n",
    "                candidate_position = src\n",
    "        elif action == \"s\":\n",
    "            candidate_position = src - env.num_cols\n",
    "            if candidate_position < 0+(dimension-1)*env.num_cells:\n",
    "                candidate_position = src\n",
    "        elif action == \"w\":\n",
    "            candidate_position = src - 1\n",
    "            if candidate_position % env.num_cols >= env.num_cols - 1:\n",
    "                candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "        pro.append(prob1)\n",
    "\n",
    "        candidate_position = src + env.num_cols\n",
    "        if candidate_position >= env.num_cells * dimension:\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        candidate_position = src + 1\n",
    "        if candidate_position % env.num_cols <= 0:  # The %-operator denotes \"modulo\"-division.\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        candidate_position = src - env.num_cols\n",
    "        if candidate_position < 0+(dimension-1)*env.num_cells:\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        candidate_position = src - 1\n",
    "        if candidate_position % env.num_cols >= env.num_cols - 1:\n",
    "            candidate_position = src\n",
    "        step.append(candidate_position)\n",
    "\n",
    "        pro.extend([prob2]*4)\n",
    "        return step,pro\n",
    "\n",
    "\n",
    "def value_iteration(agent, max_iter=-1):\n",
    "    \"\"\"\n",
    "    This function is the main process of value iteration\n",
    "\n",
    "    :param obj agent: TableAgent\n",
    "    :param int max_iter: maximum iteration.\n",
    "    :return (agent.pi,agent.value_pi), agent.pi: policy, agent.value_pi:value\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        # save the value function (we need to compare with new one later.)\n",
    "        new_value_pi = np.zeros_like(agent.value_pi)\n",
    "        for i in range(0, agent.s_len):\n",
    "            value_sas = []\n",
    "            for j in range(0, agent.a_len):\n",
    "                # for each s and a, calculate value function\n",
    "                value_sa = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "                value_sas.append(value_sa)\n",
    "            # find the best value funcion from every col.\n",
    "            new_value_pi[i] = max(value_sas)\n",
    "\n",
    "        # if the difference between the new value function and old one is less than 1e10, stop iteration.\n",
    "        diff = np.sqrt(np.sum(np.power(agent.value_pi - new_value_pi, 2)))\n",
    "        if diff < 1e-10:\n",
    "            break\n",
    "        else:\n",
    "            agent.value_pi = new_value_pi\n",
    "        if iteration == max_iter:\n",
    "            break\n",
    "            \n",
    "    for i in range(1, agent.s_len):\n",
    "        for j in range(0, agent.a_len):\n",
    "            # calculate the states - actions function\n",
    "            agent.value_q[i, j] = np.dot(agent.p[j, i, :], agent.r + agent.gamma * agent.value_pi)\n",
    "        # find the maximum action\n",
    "        max_act = np.argmax(agent.value_q[i, :])\n",
    "        agent.pi[i] = max_act\n",
    "    return agent.pi,agent.value_pi\n",
    "\n",
    "GW = Gridworld()\n",
    "Agent = TableAgent(GW)\n",
    "result = value_iteration(Agent)\n",
    "policy = np.array([Gridworld().actions[x] for x in result[0][:25]])\n",
    "v = np.array(result[1][:25])\n",
    "\n",
    "#print(np.flip(policy.reshape((5, 5)), 0))\n",
    "#print(np.flip(v.reshape((5, 5)), 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf587ef9d694513182a8ca1c721200d7",
     "grade": true,
     "grade_id": "cw1_value_iteration_two_gold_tests",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
